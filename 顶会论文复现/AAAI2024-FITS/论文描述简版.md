### 1. TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting
- **论文链接**: [https://arxiv.org/abs/2306.09364](https://arxiv.org/abs/2306.09364)
- **代码链接**: [https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer](https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer)
- **一句话总结**: 提出了 TSMixer,这是一种轻量级神经架构,专门由多层感知器(MLP)模块组成,用于修补时间序列的多元预测和表示学习.
- **研究内容**: 
  - Transformer 因其捕获长序列交互的能力而在时间序列预测中广受欢迎.然而,它们对内存和计算的高要求给长期预测带来了关键瓶颈.
  - 为了解决这个问题,我们提出了 **TSMixer**,这是一种轻量级神经架构,专门由多层感知器(MLP)模块组成,用于修补时间序列的多元预测和表示学习.
  - 受到 **MLP-Mixer** 在计算机视觉领域成功的启发,我们对其进行了调整以适应时间序列,解决挑战并引入经过验证的组件以提高准确性.
  - 这包括一种新颖的设计范例,将 **在线协调头** 附加到 MLP-Mixer 主干,用于显式建模时间序列属性,例如层次结构和通道相关性.
  - 我们还提出了一种新颖的 **混合通道建模** 和 **简单门控方法** 的注入,以有效处理跨不同数据集的噪声通道交互和泛化.
  - 通过整合这些轻量级组件,我们显着增强了简单 MLP 结构的学习能力,以最少的计算使用量超越了复杂的 Transformer 模型.
  - 此外,TSMixer 的模块化设计能够与监督学习方法和屏蔽自监督学习方法兼容,使其成为时间序列基础模型的一个有前景的构建块.
  - **TSMixer 在预测方面比最先进的 MLP 和 Transformer 模型高出 8-60%.它还优于 Patch-Transformer 模型的最新强大基准(1-2%),内存和运行时间显着减少(2-3 倍).**