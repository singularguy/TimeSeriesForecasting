以下是为你标记重点内容后的文档：

# TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting论文精读

## 一、应用背景
### （一）时间序列预测的重要性与挑战
**多变量时间序列预测在天气预报、交通预测、工业过程控制等众多领域有着广泛应用**。过去虽已有多种统计和机器学习方法对其进行研究，但仍面临诸多挑战，如*数据的复杂性、长期依赖关系的捕捉、噪声影响等*。

### （二）Transformer模型的局限
基于Transformer的模型在处理时间序列时，虽能捕捉长序列依赖，但存在诸多问题。其*位置编码导致的时间信息丢失*，尽管有诸多改进，该问题仍影响模型性能。*计算资源消耗大，内存和计算复杂度高*，限制了其在长序列预测中的应用。且在时间序列领域未充分发挥其在NLP领域的成功优势，如DLinear等简单线性模型在某些情况下能超越复杂的Transformer模型。

### （三）PatchTST等模型的不足
PatchTST虽通过分块处理提高了性能，但采用的纯通道独立方法*未明确捕获跨通道相关性*，底层的多头自注意力机制计算昂贵。其他模型如CrossFormer在通道混合技术上也存在类似问题，*难以有效处理通道间的噪声交互*。

### （四）MLP-Mixer的潜力与适应性
受计算机视觉领域中MLP-Mixer模型成功的启发，其*轻量级、快速且无需复杂自注意力机制的特点使其有望适用于时间序列问题*。但直接应用仍需改进，以更好地适应时间序列数据的特性，如*数据的动态性、长期依赖性和多通道相关性等*。

## 二、所用数据集
### （一）数据集介绍
论文使用了7个公开的多元数据集进行实验，涵盖气象、交通、电力等领域，这些数据集在时间序列预测领域被广泛使用。
1. **Weather数据集**：收集了21个气象指标，如湿度和气温等，数据来自https://www.bgc-jena.mpg.de/wetter/。
2. **Traffic数据集**：报告了旧金山高速公路不同传感器的道路占用率，数据可从https://pems.dot.ca.gov/获取。
3. **Electricity数据集**：记录了321个客户的每小时电力消耗，来源为https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014。
4. **ETT（Electricity Transformer Temperature）数据集**：包括ETTH1、ETTH2、ETTM1、ETTM2，报告了两个不同分辨率（15分钟和1小时）的电力变压器的传感器细节，可从https://github.com/zhouhaoyi/ETDataset获取。

### （二）数据集统计信息
各数据集的特征和时间步长统计信息如下表所示：

| Datasets | Weather | Traffic | Electricity | ETTH1 | ETTH2 | ETTM1 | ETTM2 |
|---|---|---|---|---|---|---|---|
| Features | 21 | 862 | 321 | 7 | 7 | 7 | 7 |
| Timesteps | 52696 | 17544 | 26304 | 17420 | 17420 | 69680 | 69680 |

## 三、用于时间序列预测的模型
### （一）TSMixer架构概述
1. **基于补丁的处理方式**
与PatchTST类似，TSMixer将输入时间序列分割为重叠或非重叠的补丁，以减少输入模型的令牌数量，提高运行时性能。在自监督训练中，补丁需严格非重叠。设输入序列长度为\(sl\)，补丁长度为\(pl\)，步长为\(s\)，则补丁数量\(n=\lfloor\frac{sl - pl}{s}\rfloor + 1\)。例如，若\(sl = 10\)，\(pl = 3\)，\(s = 2\)，则\(n=\lfloor\frac{10 - 3}{2}\rfloor + 1 = 4\)。输入数据从形状\(X_{b × sl × c}\)被重塑为\(X_{b × n × pl × c}\)，然后置换为\(X_{b × c × n × pl}\)后输入骨干网络。
2. **多种骨干网络**
    - **香草骨干（V - TSMixer）**：将所有通道扁平化处理后传递到下一层，类似视觉MLP混合技术中的做法，作为模型的简单基线。
    - **通道独立骨干（CI - TSMixer）**：受PatchTST模型启发，在MLP Mixer层中跨通道共享可学习权重，减少模型参数，可用于多数据集自监督建模，而V - TSMixer无法实现此功能。
    - **通道间骨干（IC - TSMixer）**：在骨干中额外激活通道间混合器模块，专门用于明确捕获通道间的依赖关系，进一步增强模型对多变量数据中通道相关性的建模能力。
所有骨干网络均以线性补丁嵌入层开始，该层将每个补丁独立转换为嵌入（\(X_{b × c × n × hf}^{E}=A(X^{P'})\)）。对于CI - TSMixer和IC - TSMixer，权重和偏差在通道间共享，但V - TSMixer由于通道扁平化处理，其嵌入层没有多通道概念（\(c = 1\)）。
3. **MLP Mixer层**
骨干网络堆叠一系列Mixer层，每个Mixer层尝试学习不同方向的相关性，包括不同补丁间、补丁内隐藏特征间以及不同通道间（仅IC - TSMixer包含通道间混合器）。这些混合器模块配备了MLP块、层归一化、残差连接和门控注意力。
    - 中间补丁混合器模块使用共享MLP（权重维度\(n × n\)）学习不同补丁间的相关性。
    - 补丁内混合器模块的共享MLP层（权重矩阵维度\(hf × hf\)）混合隐藏特征维度。
    - 通道间混合器（权重矩阵\(size = c × c\)）则尝试捕获多变量上下文中多个通道间的相关性（在IC - TSMixer中）。
门控注意力机制在MLP块后，根据特征值对重要特征进行概率性上采样和不重要特征的下采样，有效过滤掉不重要的特征，使模型能够聚焦于关键信息，增强了模型对长序列交互的建模能力，且无需复杂的多头自注意力机制。设输入到门控注意力模块的特征为\(X^{M}\)，经线性变换\(A(X^{M})\)后通过softmax得到注意力权重\(W_{b × c × n × hf}^{A}=softmax(A(X^{M}))\)，输出\(X^{G}=W^{A}·X^{M}\)。
4. **模型头**
    - **预测头（Prediction Head）**：在监督训练中使用，将骨干网络输出的隐藏特征扁平化后，经简单线性层（包含随机失活操作）将其转换为预测的多变量时间序列（\(\hat{Y}_{b × fl × c}\)），输出模型对未来时间序列值的预测结果。设预测头的线性层为\(L_{pred}(\cdot)\)，则\(\hat{Y}_{b × fl × c}=L_{pred}(Flatten(X^{G}))\)，其中\(Flatten(\cdot)\)表示扁平化操作。
    - **预训练头（Pretrain Head）**：在自监督训练的预训练阶段使用，对扁平化后的隐藏特征进行处理，通过线性层输出一个与输入维度相同的多变量系列（\(\hat{X}_{b × sl × c}\)），用于在自监督任务中重建输入数据，帮助模型学习数据的通用表示。设预训练头的线性层为\(L_{pretrain}(\cdot)\)，则\(\hat{X}_{b × sl × c}=L_{pretrain}(Flatten(X^{G}))\)。默认情况下，预测头和预训练头在通道间共享权重。
5. **在线预测协调**
    - **交叉通道预测协调头（Cross - channel forecast reconciliation head）**：为解决通道间预测依赖问题而设计，例如在零售领域中，某一通道在某一时间的预测可能依赖于另一通道在不同时间的预测（如销售与折扣模式的关系）。将每个预测点通过附加其前后周围的预测点（基于上下文长度\(cl\)）转换为补丁（长度为\(spl\)），然后将每个补丁在通道上扁平化，并通过门控注意力和线性层处理，得到该补丁的修订预测点，使所有通道在周围上下文的预测通道值基础上协调其预测值，实现有效的跨通道建模。残差连接确保在通道相关性存在噪声的情况下，协调过程不会导致精度下降。这种“混合”方法（将通道独立骨干与交叉通道协调头相结合）在处理通道间噪声交互方面表现出较好的稳定性，相比其他通道混合方法具有优势，同时有助于骨干网络在不同通道数量的多个数据集上进行泛化训练，因为它将通道相关性建模任务转移到了预测头（依赖于任务和数据）。
    - **在线层次补丁协调头（Online hierarchical patch reconciliation head）**：考虑到时间序列数据常具有内在的层次结构（如天气预测中，每日预测可聚合为每周预测；销售预测中，单个商店预测可汇总为区域或国家层面的预测），该协调头旨在自动推导层次补丁聚合损失，并在训练过程中与颗粒级预测误差一起最小化。原始预测\(\hat{Y}\)被分割为\(op\)个长度为\(pl\)的补丁（表示为\(\hat{Y}^{P}\)），同时\(\hat{Y}\)通过线性层预测补丁级的层次聚合预测\(\hat{H}\)。然后将\(\hat{Y}^{P}\)和\(\hat{H}\)在补丁级别上连接，并通过另一个线性变换得到协调后的颗粒级预测\(\hat{Y}_{rec}\)，使颗粒级预测在补丁级别上根据补丁聚合预测进行协调，从而提高颗粒级预测的准确性。残差连接确保在预测聚合信号具有挑战性的情况下，协调过程不会导致精度下降。

### （二）训练方法
1. **监督训练**
遵循“预测”工作流程，输入历史时间序列首先经过归一化、分块和置换等预处理步骤，然后进入TSMixer骨干网络进行主要的学习过程，骨干网络输出的特征再通过预测头转换为基础预测\(\hat{Y}\)。模型通过最小化基础预测与真实未来值\(Y\)之间的均方误差（MSE）来进行训练，即\(L(Y,\hat{Y})=\|Y-\hat{Y}\|_{2}^{2}\)。在训练过程中，可以选择激活在线预测协调头（交叉通道或层次补丁协调头），此时将使用定制的基于MSE的目标函数对调整后的预测进行优化。
2. **自监督训练**
分两阶段进行。第一阶段是预训练，模型使用掩码时间序列建模（MTSM）任务进行预训练，在这个过程中，随机对一部分输入补丁应用掩码，模型学习从无掩码的输入补丁中恢复被掩码的补丁，通过最小化掩码补丁的MSE重建误差来优化模型。预训练阶段的数据预处理与预测工作流程中的相同，但在自监督训练中，为避免重叠补丁，使用了更小的补丁长度和相同大小的步长（\(pl = 8\)，\(s = 8\)），相应地更新了隐藏特征和扩展特征大小（\(hf = 16\)，\(ef = 32\)）。第二阶段是微调，预训练后的模型通过“预测”工作流程针对监督下游任务进行微调，在微调的初始阶段（前20个epoch），冻结骨干网络权重仅训练头部（线性探测），之后再对整个网络（骨干网络 + 头部）进行100个epoch的微调和优化，最终模型根据在验证集上的最佳得分进行选择。

## 四、评价预测效果的指标
1. **均方误差（MSE）**
用于衡量预测值与真实值之间的平均平方误差，是评估预测准确性的常用指标。在论文中，通过计算模型预测值与真实未来值之间的MSE，比较不同模型和变体的性能，*MSE值越小表示预测准确性越高*。
2. **平均绝对误差（MAE）**
衡量预测值与真实值之间的平均绝对误差，与MSE类似，用于评估预测的准确性。论文中虽主要以MSE解释结果，但也提及了MAE指标，*两者在评估模型性能时呈现相似的相对模式*。

## 五、结论
### （一）模型优势
TSMixer是一种专门为多变量时间序列预测和表示学习设计的纯MLP架构，通过引入创新的混合架构，包括各种协调头和门控注意力机制，显著增强了简单MLP结构的学习能力，使其能够超越复杂的Transformer模型。

### （二）性能表现
在7个流行的公共数据集上进行的详细实验表明，TSMixer在准确性和计算效率方面均优于现有基准模型。与DLinear、PatchTST等模型相比，TSMixer在预测准确性上有显著提升（如*相对于DLinear提升8%，相对于PatchTST提升1 - 2%*），同时在训练时间和内存使用方面有显著减少（如*相对于PatchTST训练时间和内存使用减少2 - 3倍*）。在自监督表示学习的多元预测中，TSMixer也取得了显著改进，相较于现有自监督基准模型提升50 - 70%，且优于自监督PatchTST。

### （三）未来展望
计划将TSMixer扩展到其他下游任务（如分类、异常检测等），并提高跨数据集的迁移学习能力。此外，还将研究Swin、Shift和其他新的Mixer变体在时间序列中的适用性，以进一步提升模型性能和应用范围。 